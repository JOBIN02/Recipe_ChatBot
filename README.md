# Name Matching System

This project implements a name-matching system that finds the most similar names from a dataset when a user provides an input name.
It uses string similarity techniques to calculate and rank the closest matches.

**Features**

Accepts a dataset of at least 30 names (e.g., Geetha, Gita, Gitu, etc.).

Matches user input against the dataset.

**Returns:**

Best Match: The closest name with the highest similarity score.

Ranked Matches: A list of other similar names with scores.

Can be extended using fuzzy matching libraries or Vector DB search.

**Tech Stack**

Python 3.8+

**Libraries:**

fuzzywuzzy (or rapidfuzz) for similarity scoring

pandas (optional, for structured output)





# Recipe_ChatBot

This project provides a complete, locally-runnable chatbot that suggests recipes based on ingredients you provide. It uses an open-source Large Language Model (LLM) served by Ollama, a Python FastAPI backend, and a simple web-based chat interface.

Project Architecture
The project works in three main parts:

LLM Server (Ollama): We use Ollama to download, manage, and serve a powerful open-source LLM (like Llama 3) on your local machine. Ollama exposes the model through a local API.

Backend API (FastAPI): A Python server built with FastAPI acts as the middle layer. It receives requests from the web interface, formats a detailed prompt for the LLM, sends it to the Ollama API, and streams the response back to the user.

Frontend (HTML & JavaScript): A single HTML file creates a user-friendly chat interface. It sends the user's ingredients to the FastAPI backend and displays the recipe suggestion as it's generated by the model in real-time.

Key Deliverables Checklist

1.Server Setup: Instructions to install and serve an open-source model using Ollama.

2. Fine-Tuning Data: A sample recipes.jsonl dataset is provided. While this project uses prompt engineering for simplicity, this file can be used for actual fine-tuning with tools like ollama create.

3. API Integration: The FastAPI backend exposes a /generate_recipe endpoint that returns a JSON-based streaming response.

4. Chatbot Development: A web UI is provided in index.html.

5. Runnable Code: The project is designed to be fully runnable on a standard laptop.

6. Documentation: This README provides all necessary instructions.

Setup and Installation
Follow these steps to get the project running.

Step 1: Install Ollama and Download a Model
You need to have an LLM running locally. Ollama is the easiest way to do this.

Install Ollama: Go to https://ollama.com/ and download the installer for your operating system (Windows, macOS, or Linux).

Pull a Model: Once Ollama is installed and running, open your terminal or command prompt and pull a model. We recommend llama3:8b, which offers a great balance of performance and resource usage.

ollama pull llama3:8b

Verify Ollama is Running: The Ollama application should be running in the background. You can check its status from your system's taskbar or menu bar. The API will be available at http://localhost:11434.

Step 2: Set Up the Python Backend
Prerequisites: Make sure you have Python 3.8 or newer installed.

Create a Virtual Environment (Recommended):

# Create the environment
python -m venv venv

 Activate it
On Windows:
.\venv\Scripts\activate
 On macOS/Linux:
source venv/bin/activate

Install Dependencies: The required Python packages are listed in requirements.txt. Install them using pip.

pip install -r requirements.txt

**How to Run the Project**

**Step 1:** Start the FastAPI Backend Server
Make sure your virtual environment is activated. Run the following command from the project's root directory:

uvicorn main:app --reload

The server will start, and you'll see a message indicating it's running on http://127.0.0.1:8000.

**Step 2:** Open the Chatbot Interface
Navigate to the project directory in your file explorer.

Double-click the index.html file to open it in your web browser.

How to Use the Chatbot
Once the web page is open, you will see a chat interface.

In the input box at the bottom, type the ingredients you have (e.g., "chicken, rice, tomatoes, onions").

Press Enter or click the send button.

The chatbot will process your request and stream back a recipe suggestion.

Sample Input and Expected Output
Sample Input:

Eggs, Onions, Bell Peppers

Expected Output:

The chatbot will generate a detailed recipe for something like a "Classic Bell Pepper and Onion Omelette," including a list of ingredients, step-by-step instructions, and maybe some cooking tips. The response will appear token-by-token, like a real conversation.

This completes the setup. You now have a fully functional, local AI recipe assistant
